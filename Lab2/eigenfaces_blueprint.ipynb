{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "# Eigenfaces and Principal Component Analysis (PCA)\n",
    "\n",
    "In this notebook, we will explore the concept of Eigenfaces, which are eigenvectors used in the computer vision problem of human face recognition. We will see how PCA can be applied to high-dimensional data like images to reduce the dimensionality while preserving as much variability as possible.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the steps involved in performing PCA on image data.\n",
    "- Learn how to reconstruct images from their reduced dimensional representations.\n",
    "- Visualize how different numbers of components affect the reconstruction quality.\n",
    "\n",
    "Please execute each cell in sequence and follow along with the comments and markdown cells for a better understanding of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_and_process_images(relative_path: str, num_images_to_display: int = 10, image_size: tuple = (120, 120)):\n",
    "    \"\"\"\n",
    "    Display a specified number of images from a folder in a grid and prepare all images for PCA.\n",
    "\n",
    "    Args:\n",
    "        - folder_path (str): relative path to the folder containing the images.\n",
    "        - num_images_to_display (int): Number of images to display in a grid.\n",
    "        - image_size (tuple): Size to which images should be resized (width, height).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the list of all original images, the numpy array of images for PCA, and the mean image.\n",
    "    \"\"\"\n",
    "    original_images = []\n",
    "    input_vector = []\n",
    "    displayed_images = 0\n",
    "    folder_path = os.getcwd() + relative_path\n",
    "\n",
    "    # Determine the grid size for plotting the specified number of images\n",
    "    grid_size = int(np.ceil(np.sqrt(num_images_to_display)))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 2, grid_size * 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image files\n",
    "            with Image.open(os.path.join(folder_path, filename)) as img:\n",
    "                # Resize image and convert to numpy array\n",
    "                img_resized = img.resize(image_size)\n",
    "                img_array = np.array(img_resized)\n",
    "                # Append original images and flattened image to the lists\n",
    "                original_images.append(img_array)\n",
    "                input_vector.append(img_array.flatten())\n",
    "                # Display the image if within the specified number\n",
    "                if displayed_images < num_images_to_display:\n",
    "                    axes[displayed_images].imshow(img_array)\n",
    "                    axes[displayed_images].axis('off')  # Hide the axes\n",
    "                    displayed_images += 1\n",
    "\n",
    "    # Hide any remaining axes without images\n",
    "    for ax in axes[displayed_images:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Convert the list of images to a 2D numpy array for PCA\n",
    "    data = np.array(input_vector)\n",
    "    # Calculate the mean image\n",
    "    mean_img = np.mean(data, axis=0)\n",
    "\n",
    "    return original_images, data, mean_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sample Images\n",
    "\n",
    "Before we dive into PCA, let's take a look at some sample images from our dataset. It's important to understand the kind of data we are working with. In the following cell, we will display some images of faces that we will later use to extract eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to dataset\n",
    "# images_path = \"/archive/img_align_celeba/img_align_celeba\"\n",
    "images_path = \"/dataset_500\"\n",
    "\n",
    "# Set the image size (the bigger, the more it will take to perform pca)\n",
    "image_size = (50, 50)\n",
    "\n",
    "# Define the number of images to display on the grid\n",
    "num_images = 16\n",
    "\n",
    "# Create the dataset and visualize some samples\n",
    "original_images, input_vector, mean_img = display_and_process_images(relative_path=images_path, num_images_to_display=num_images, image_size=image_size)\n",
    "\n",
    "# Verify the shape of the input vector\n",
    "input_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) for Eigenfaces\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical procedure that utilizes an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. When applied to facial images, the principal components are known as 'eigenfaces', which are the directions in which the images differ the most.\n",
    "\n",
    "### PCA Process Explained\n",
    "\n",
    "1. **Normalization**: The first step in PCA is to normalize the data. This involves subtracting the mean from each feature. The mean subtraction ensures that the PCA looks for patterns in the variance of the data, centralizing the data around the origin in the multidimensional space.\n",
    "\n",
    "2. **Covariance Matrix**: The next step is to calculate the covariance matrix of the normalized data. The covariance matrix captures the variance and the relationship between the different dimensions of the data. It is a key component in understanding how dimensions correlate with each other.\n",
    "\n",
    "3. **Eigenvalues and Eigenvectors**: We then compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvalues tell us how much variance there is in the data for each eigenvector direction. The eigenvectors, which are the principal components in PCA, give us the directions of maximum variance.\n",
    "\n",
    "4. **Sorting**: The eigenvalues and corresponding eigenvectors are sorted in descending order. This is done because we are often interested in the principal components that explain the most variance, which are associated with the largest eigenvalues.\n",
    "\n",
    "5. **Component Selection**: We select the top `num_components` eigenvectors as our principal components. This is the dimensionality reduction step where we choose to keep only the features that contain the most information about our dataset.\n",
    "\n",
    "6. **Projection**: Finally, the data is projected onto the new feature space formed by the selected eigenvectors. This results in a transformed dataset where each row is now represented in terms of the principal components.\n",
    "\n",
    "### Outputs of the PCA Function\n",
    "\n",
    "The function returns a tuple with three elements:\n",
    "\n",
    "- **Projected Data**: This is the original data transformed into the new feature space. Each observation is now expressed as a combination of the top principal components. We would expect this to have the same number of rows as the original data but with `num_components` columns.\n",
    "\n",
    "- **Eigenvalues**: These values measure the amount of variance explained by each of the principal components. We expect the eigenvalues to be sorted in descending order, with the largest values first.\n",
    "\n",
    "- **Eigenvectors**: Also known as the principal components, these are the directions in which the data varies the most. The eigenvectors are orthogonal to each other and form the basis of the new feature space. We expect the eigenvectors to be sorted according to their corresponding eigenvalues.\n",
    "\n",
    "By understanding the PCA process and its outputs, we gain valuable insights into the structure and variability of our data, which is particularly useful in the field of computer vision for tasks like facial recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data: np.array, num_components: int):\n",
    "    \"\"\"\n",
    "    Performs Principal Component Analysis (PCA) on the given data.\n",
    "    \n",
    "    Args:\n",
    "        - data: A 2D numpy array where each row represents an observation and each column a feature.\n",
    "        - num_components: The number of principal components to retain.\n",
    "    Returns\n",
    "        A tuple containing the projected data, eigenvalues, and eigenvectors.\n",
    "    \"\"\"\n",
    "    # Normalize the data by subtracting the mean\n",
    "    mean = np.mean(data, axis=0)\n",
    "    normalized_data = data - mean\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(normalized_data, rowvar=False)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "\n",
    "    # Sort eigenvectors by eigenvalues in descending order\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Select the top 'num_components' eigenvectors\n",
    "    eigenvectors = eigenvectors[:, :num_components]\n",
    "\n",
    "    # Project the data onto the new feature space\n",
    "    projected_data = np.dot(normalized_data, eigenvectors)\n",
    "\n",
    "    return projected_data, eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvector = latent\n",
    "# eigenvalues = coeff\n",
    "\n",
    "# Set the number of components to extract\n",
    "num_components = 120\n",
    "# Perform PCA to project the data and extract both eigen values and vectors\n",
    "projected_data, eigenvalues, eigenvectors = pca(input_vector, num_components=num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(projected_data), len(eigenvalues),len(eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Explained Variance in PCA\n",
    "\n",
    "The `plot_explained_variance` function is designed to help visualize the concept of explained variance in the context of Principal Component Analysis (PCA). By plotting both the individual and cumulative explained variance for each principal component, we can assess how many components are required to capture most of the information in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance(eigenvalues: int, num_components_to_display: int = 120):\n",
    "    \"\"\"\n",
    "    Plot the explained variance by each principal component with improvements.\n",
    "\n",
    "    - eigenvalues: The eigenvalues obtained from PCA.\n",
    "    - num_components_to_display: Number of principal components to display on the plot.\n",
    "    \"\"\"\n",
    "    # Calculate the total variance\n",
    "    total_variance = sum(eigenvalues)\n",
    "    # Calculate the explained variance by each component\n",
    "    var_explained = [(i / total_variance) * 100 for i in sorted(eigenvalues, reverse=True)]\n",
    "    # Calculate the cumulative variance\n",
    "    cum_var_explained = np.cumsum(var_explained)\n",
    "\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot individual explained variance\n",
    "    ax.bar(range(1, num_components_to_display + 1), var_explained[:num_components_to_display], alpha=0.5, align='center', label='Individual explained variance')\n",
    "\n",
    "    # Plot cumulative explained variance\n",
    "    ax.plot(range(1, num_components_to_display + 1), cum_var_explained[:num_components_to_display], 'r-', lw=2, label='Cumulative explained variance')\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_ylabel('Explained Variance Percentage')\n",
    "    ax.set_xlabel('Principal Component Index')\n",
    "    ax.set_title('Explained Variance by PCA Components')\n",
    "\n",
    "    # Set log scale if needed\n",
    "    # ax.set_yscale('log')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Tight layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot with a tight layout\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Interpretation\n",
    "\n",
    "The displayed graph typically consists of two key elements:\n",
    "1. **Bar Plot (Blue Bars)**: Each bar represents the variance explained by an individual principal component. The height of the bar indicates the percentage of the total variance that the corresponding principal component accounts for.\n",
    "\n",
    "2. **Line Plot (Red Line)**: This shows the cumulative explained variance. It indicates the total variance explained as we include more and more principal components. The `y`-value of the line plot at any given index `x` shows the total percentage of variance explained by the first `x` components.\n",
    "\n",
    "### Insights from the Graph\n",
    "\n",
    "- A steep curve at the beginning of the line plot indicates that the first few components explain a significant portion of the variance.\n",
    "- The point at which the line plot starts to plateau suggests that adding more components does not significantly increase the explained variance, which can inform the choice of how many components to use for data reconstruction or further analysis.\n",
    "- Ideally, we want to choose a number of components that captures a high percentage of the variance while also simplifying the dataset by reducing its dimensionality.\n",
    "\n",
    "This visualization aids in making informed decisions about the trade-off between data compression and information retention when performing PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_explained_variance(eigenvalues=eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the Mean Face and Eigenfaces\n",
    "\n",
    "After performing Principal Component Analysis (PCA) on our dataset, we are left with eigenvectors that represent the principal components. These components are essentially the \"eigenfaces,\" which are the directions of the greatest variance within our face dataset.\n",
    "\n",
    "### The Mean Face\n",
    "\n",
    "The mean face is computed by averaging each pixel across all images. This gives us a baseline face that represents the average features of all the faces in the dataset.\n",
    "\n",
    "### Eigenfaces\n",
    "\n",
    "Each eigenvector from the PCA can be visualized as an eigenface. These eigenfaces highlight the most significant features that can reconstruct all the faces in the dataset when combined. They are the essence of what PCA is capturing about the facial data.\n",
    "\n",
    "Let's go through the process of visualizing these eigenfaces:\n",
    "\n",
    "1. **Calculate the Mean Face**: We start by computing the mean face, which is the average of all faces in the dataset.\n",
    "2. **Display the Mean Face**: The mean face is then displayed to give us an idea of the average features.\n",
    "3. **Normalize and Display Eigenfaces**: For each eigenface, we normalize its pixel values to the range of 0-255 to ensure proper visualization. Each eigenface is reshaped back to the original dimensions of the images and displayed.\n",
    "\n",
    "These eigenfaces can be used for various applications such as face recognition, face reconstruction, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mean_and_eigenvectors(input_vector: np.array,\n",
    "                                  eigenvectors: np.array,\n",
    "                                  image_shape: tuple = (120, 120, 3),\n",
    "                                  num_components_to_display: int = 10):\n",
    "    \"\"\"\n",
    "    Display the mean image and a grid of eigenvectors (eigenfaces) of the dataset.\n",
    "    \n",
    "    Args:\n",
    "        - input_vector (numpy.ndarray): The array of flattened images.\n",
    "        - eigenvectors (numpy.ndarray): The matrix of eigenvectors computed from PCA.\n",
    "        - image_shape (tuple): The shape to reshape the flattened images back into.\n",
    "        - num_components_to_display (int): The number of principal components to display in a grid.\n",
    "    \"\"\"\n",
    "    # Calculate the mean image from the input vector\n",
    "    mean_image = np.mean(input_vector, axis=0)\n",
    "\n",
    "    # Display the mean image\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(Image.fromarray(np.uint8(mean_image.reshape(image_shape))))\n",
    "    plt.title('Mean Face')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Determine the number of rows and columns for the grid\n",
    "    grid_cols = int(np.ceil(np.sqrt(num_components_to_display)))\n",
    "    grid_rows = int(np.ceil(num_components_to_display / grid_cols))\n",
    "\n",
    "    # Create a figure with subplots in a grid\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(15, 15))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "    # Display each eigenvector as an eigenface in the grid\n",
    "    for i in range(num_components_to_display):\n",
    "        eigen_vec = eigenvectors[:, i]\n",
    "        # Normalize the eigenvector to be in the range [0, 255] for display\n",
    "        normalized_eigen_vec = 255 * (eigen_vec - np.min(eigen_vec)) / (np.max(eigen_vec) - np.min(eigen_vec))\n",
    "        # Reshape the eigenvector to the original image shape\n",
    "        eigenface = normalized_eigen_vec.reshape(image_shape)\n",
    "        # Display the eigenface\n",
    "        ax = axes[i]\n",
    "        ax.imshow(Image.fromarray(np.uint8(eigenface)), cmap='gray')\n",
    "        ax.set_title(f'Eigenface {i+1}')\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "    # Hide any unused axes if the number of components is not a perfect square\n",
    "    for i in range(num_components_to_display, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the mean image and a grid of eigenvectors (eigenfaces) of the dataset\n",
    "display_mean_and_eigenvectors(input_vector=input_vector, eigenvectors=eigenvectors, image_shape=(50, 50, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modes of Variation in PCA\n",
    "\n",
    "In PCA, the eigenvectors represent the directions of maximum variance in the dataset, often called the \"principal components.\" By altering these components, we can observe how they affect the overall data reconstruction.\n",
    "\n",
    "### Functionality of `display_modes_of_variation`\n",
    "\n",
    "The `display_modes_of_variation` function visualizes how changes along a single principal component can alter the reconstructed image. This is achieved by scaling the selected eigenvector (from the PCA output) with a range of standard deviations and adding this to the mean image.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Standard Deviation Range**: The function takes a list of standard deviations to apply to the eigenvector.\n",
    "2. **Scaling the Eigenvector**: Each eigenvector is multiplied by the corresponding standard deviation and the square root of its eigenvalue.\n",
    "3. **Adding Variation**: The scaled eigenvector is added to the mean image to create a varied image.\n",
    "4. **Normalization**: The resulting varied image is clipped to ensure the pixel values are within the valid range for display.\n",
    "5. **Grid Display**: The function plots these variations in a grid layout, where each subplot corresponds to a different standard deviation, showcasing the effect of that variation.\n",
    "\n",
    "Through this visualization, students can better understand the influence of individual principal components on the data and the concept of variance in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_modes_of_variation(mean_img: np.array, eigenvectors: np.array, eigenvalues: np.array, comp: int = 4, image_shape: tuple = (50, 50, 3), std_devs: np.array = np.arange(-3.0, 3.25, 0.25)):\n",
    "    \"\"\"\n",
    "    Display modes of variation by adding scaled eigenvectors to the mean image.\n",
    "\n",
    "    Args:\n",
    "        mean_img (numpy.ndarray): The mean of the original flattened images.\n",
    "        eigenvectors (numpy.ndarray): Matrix where each column is an eigenvector.\n",
    "        eigenvalues (numpy.ndarray): Array of eigenvalues corresponding to the eigenvectors.\n",
    "        comp (int): The index of the eigenvector to vary.\n",
    "        image_shape (tuple): The shape to convert flattened images back into.\n",
    "        std_devs (list or numpy.ndarray): The standard deviations to scale the eigenvector.\n",
    "    \"\"\"\n",
    "    # Determine the grid size for plotting\n",
    "    grid_size = int(np.ceil(np.sqrt(len(std_devs))))\n",
    "    \n",
    "    # Create a figure with subplots in a grid\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 2, grid_size * 2))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "    # Loop through each standard deviation\n",
    "    for i, std in enumerate(std_devs):\n",
    "        # Scale the chosen eigenvector by the current standard deviation and eigenvalue\n",
    "        variation = eigenvectors[:, comp] * std * np.sqrt(eigenvalues[comp])\n",
    "        # Add the variation to the mean image\n",
    "        varied_image = (mean_img + variation).reshape(image_shape)\n",
    "        # Normalize the image for proper display\n",
    "        varied_image = np.clip(varied_image, 0, 255)\n",
    "        \n",
    "        # Display the varied image\n",
    "        axes[i].imshow(Image.fromarray(np.uint8(varied_image)), cmap='gray')\n",
    "        axes[i].set_title(f'{std:.2f} STDs')\n",
    "        axes[i].axis('off')  # Hide axes for a cleaner look\n",
    "\n",
    "    # Hide any unused subplots if the number of std_devs isn't a perfect square\n",
    "    for ax in axes[len(std_devs):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the \"Modes of Variation\"\n",
    "# Calculate the mean of the images\n",
    "mean_img = np.mean(input_vector, axis=0)\n",
    "\n",
    "# The original shape of the images\n",
    "image_shape = (50, 50, 3) \n",
    "\n",
    "# Fifth eigenvector (indexing starts from 0) \n",
    "comp = 4 \n",
    "\n",
    "# A range of standard deviations, from -3 to 3\n",
    "std_devs = np.arange(-3.0, 3.25, 0.25)  \n",
    "\n",
    "display_modes_of_variation(mean_img=mean_img,\n",
    "                           eigenvectors=eigenvectors,\n",
    "                           eigenvalues=eigenvalues,\n",
    "                           image_shape=image_shape,\n",
    "                           comp=comp,\n",
    "                           std_devs=std_devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Reconstruction using Principal Components\n",
    "\n",
    "In this section, we explore the reconstruction of images using different numbers of principal components obtained from PCA. This process helps us understand how the number of components affects the quality of the reconstructed image.\n",
    "\n",
    "### The Reconstruction Process\n",
    "\n",
    "1. **Original Image**: We start by displaying the original image for comparison.\n",
    "2. **Reconstruction Loop**: We then reconstruct the image using an increasing number of principal components. This is done in a loop where each iteration uses one additional component.\n",
    "3. **Truncation of Eigenvectors**: In each iteration, we truncate the eigenvectors matrix to the current number of components.\n",
    "4. **Projection and Reconstruction**: We project the original image onto the truncated eigenspace and then reconstruct it by adding back the mean image.\n",
    "5. **Visualization in a Grid**: The original image and each reconstructed image are displayed in a grid. This allows us to visually compare how the reconstruction quality improves as more components are used.\n",
    "\n",
    "### Visual Insights\n",
    "\n",
    "By observing the reconstructions, we can see that as we increase the number of components, the reconstructed image becomes more and more similar to the original. This demonstrates the power of PCA in capturing the most significant features of the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_reconstruction_comparison(original_images: np.array,\n",
    "                                      eigenvectors: np.array,\n",
    "                                      input_vector: np.array,\n",
    "                                      mean_img: np.array,\n",
    "                                      image_shape: tuple = (50, 50, 3),\n",
    "                                      num_components=60,\n",
    "                                      selected_img: int = 0):\n",
    "    \"\"\"\n",
    "    Display the original image and its reconstructions using an increasing number of principal components.\n",
    "\n",
    "    Args:\n",
    "        - original_images (list of numpy.ndarray): The list of original images.\n",
    "        - eigenvectors (numpy.ndarray): The matrix of eigenvectors computed from PCA.\n",
    "        - input_vector (numpy.ndarray): The array of flattened images used for PCA.\n",
    "        - mean_img (numpy.ndarray): The mean of the original flattened images.\n",
    "        - image_shape (tuple): The shape to convert flattened images back into.\n",
    "        - num_components (int): The number of principal components to use for reconstructions.\n",
    "        - selected_img (int): The index of the image to be displayed\n",
    "\n",
    "    This function displays the original image and its reconstructions from a varying number of principal components.\n",
    "    It helps to visualize how the image reconstruction quality improves with more components.\n",
    "    \"\"\"\n",
    "    # Select the first image for comparison\n",
    "    original = original_images[selected_img]\n",
    "\n",
    "    # Determine the grid size for plotting\n",
    "    grid_cols = int(np.ceil(np.sqrt(num_components + 1)))  # +1 for the original image\n",
    "    grid_rows = int(np.ceil((num_components + 1) / grid_cols))\n",
    "\n",
    "    # Create a figure with subplots in a grid\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols * 2, grid_rows * 2))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "    # Display the original image in the first subplot\n",
    "    axes[0].imshow(Image.fromarray(np.uint8(original)), cmap='gray')\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Iterate over the range of components for reconstruction\n",
    "    for n_dims in range(1, num_components + 1):\n",
    "        # Truncate the eigenvectors to the current number of dimensions\n",
    "        p_trunc = eigenvectors[:, :n_dims]\n",
    "        # Project the original image onto the truncated eigenspace\n",
    "        y_k = p_trunc.T @ (input_vector[selected_img] - mean_img)\n",
    "        # Reconstruct the image from the projection\n",
    "        reconstruction_vec = mean_img + p_trunc @ y_k\n",
    "        # Reshape the reconstruction vector back into an image\n",
    "        reconstruction_img = reconstruction_vec.reshape(image_shape)\n",
    "        # Display the reconstructed image\n",
    "        axes[n_dims].imshow(Image.fromarray(np.uint8(reconstruction_img)), cmap='gray')\n",
    "        axes[n_dims].set_title(f'{n_dims} Components')\n",
    "        axes[n_dims].axis('off')\n",
    "\n",
    "    # Turn off any unused subplots\n",
    "    for ax in axes[num_components + 1:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_reconstruction_comparison(original_images=original_images,\n",
    "                                  eigenvectors=eigenvectors,\n",
    "                                  input_vector=input_vector,\n",
    "                                  image_shape=(50, 50, 3),\n",
    "                                  mean_img=mean_img,\n",
    "                                  num_components=60,\n",
    "                                  selected_img=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
