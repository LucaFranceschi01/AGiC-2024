{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection and Recognition CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Luca Franceschi (u199149)\n",
    "\n",
    "In this notebook we will try to construct two CNN models, one for face boundary detection and another for face recognition. The intention for the Coding Challenge 3 is to: with each image first detect if there is a face or not, in case there is, detect the boundaries. Then we can crop the image with the calculated boundaries and pass it to the recognition model to be able to recognize if the face exists in the dataset or is an impostor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io\n",
    "import torch.jit\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from matplotlib import ticker\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms.functional import get_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.0001                 # Learning rate\n",
    "epochs1 = 45                # Total number of epochs\n",
    "epochs2 = 20\n",
    "batch_size = 32             # The sizes in which the dataset is split to feed the networks\n",
    "det_resized = (224, 224)    # The normalized size of the images in the detection model\n",
    "rec_resized = det_resized   # The normalized size of the images in the recognition model\n",
    "\n",
    "# Other parameters\n",
    "epoch_step = 1              # Frequency in which epoch losses are printed on training\n",
    "images_path = './TRAINING/' # Relativ# train_mean, train_std = compute_mean_std(det_train_data)e path to the training dataset\n",
    "target_matrix_path = ''     # Relative path to the \"AGC_Challenge3_Training.mat\" file\n",
    "identities = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print if gpu acceleration is enabled\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The target matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the matrix we can import either the boundaries or the identity. \n",
    "- For the detection network we will import the Image name along with the boundaries. In case there is a face (or more) the first position in the boundaries will be the amount of faces in that image. In case there is no faces, that parameter will be 0 and the boundaries will be a \"padding\" of zeros. The imported data in this case will look like: ```[name, [#faces, x0, y0, x1, y1]]```, very similar as in previous labs. In case there are more than one face the first one in the matrix will be considered.\n",
    "- For the recognition network we will import the image name along with the id and its boundaries. If the id is -1 it is imported as 0 to fix some out of bounds error. This is corrected in the output of the model and is transparent to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(labels_path, labels_wanted='boundaries'):\n",
    "    mat = scipy.io.loadmat(labels_path)['AGC_Challenge3_TRAINING'][0]\n",
    "    data = []\n",
    "    # counter = 0\n",
    "    # impostors = 0\n",
    "    for entry in mat:\n",
    "        key = entry[1][0]\n",
    "        if (len(entry[2]) > 0):\n",
    "            boundary = np.array(entry[2][0], dtype=np.int32)\n",
    "            if (labels_wanted == 'boundaries'):\n",
    "                data.append([key, boundary])\n",
    "            elif (labels_wanted == 'identity'):\n",
    "                if (entry[0][0][0] == -1):\n",
    "                    # if (impostors < counter / identities):\n",
    "                    data.append([key, 0, boundary])\n",
    "                        # impostors += 1\n",
    "                else:\n",
    "                    data.append([key, entry[0][0][0], boundary])\n",
    "                    # counter += 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_data = load_data(target_matrix_path + 'AGC_Challenge3_Training.mat')\n",
    "recognition_data = load_data(target_matrix_path + 'AGC_Challenge3_Training.mat', 'identity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset):\n",
    "    image_count = len(dataset)\n",
    "    images = np.zeros((image_count, rec_resized[0], rec_resized[1], 3))\n",
    "    for i, (image_path, _) in enumerate(dataset):\n",
    "        with Image.open(images_path + image_path) as image:\n",
    "            image = image.resize(det_resized)\n",
    "            image = image.convert('RGB')\n",
    "            image_array = np.array(image)\n",
    "            images[i] = image_array\n",
    "\n",
    "    mean = np.mean(images, axis=(0, 1, 2)) / 255.0\n",
    "    std_dev = np.std(images, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "    return mean, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = compute_mean_std(detection_data)\n",
    "\n",
    "# data_mean = [0.485, 0.456, 0.406]\n",
    "# data_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_mean = m\n",
    "data_std = s\n",
    "\n",
    "print(data_mean, data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_tr_transform = transforms.Compose([\n",
    "    # transforms.Resize(300, 300),\n",
    "    transforms.RandomResizedCrop(det_resized),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=train_mean, std=train_std)\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])\n",
    "\n",
    "det_base_transform = transforms.Compose([\n",
    "    transforms.Resize(det_resized),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])\n",
    "\n",
    "rec_tr_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(rec_resized),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    # transforms.RandomRotation(180),\n",
    "    transforms.RandomGrayscale(0.2),\n",
    "    transforms.ColorJitter(0.5, 0.5, 0.5, 0.5),\n",
    "    transforms.RandomAffine(90),\n",
    "    transforms.AutoAugment(),\n",
    "    ## scaling shearing affine\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])\n",
    "\n",
    "rec_val_transform = transforms.Compose([\n",
    "    transforms.Resize(rec_resized),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper: # used in classification problem\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "        return self.counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_layers, fc_layers, type):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn_layers = cnn_layers\n",
    "        self.fc_layers = fc_layers\n",
    "        self.cnn_type = type\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.cnn_layers(data)\n",
    "        output = self.flatten(output) # before linear layer !!!\n",
    "        output = self.fc_layers(output)\n",
    "        return output\n",
    "\n",
    "    def fit(self, training_data, loss_fn, optimizer: optim.Optimizer):\n",
    "        self.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_data, target in training_data:\n",
    "            batch_data = batch_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.forward(batch_data)\n",
    "            \n",
    "            if self.cnn_type == 'detection':\n",
    "                pass\n",
    "            elif self.cnn_type == 'recognition':\n",
    "                target = target.type(torch.long)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(training_data)\n",
    "\n",
    "    def evaluate(self, validation_data, loss_fn):\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_data, target in validation_data:\n",
    "            batch_data = batch_data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = self.forward(batch_data)\n",
    "\n",
    "            if self.cnn_type == 'detection':\n",
    "                pass\n",
    "            elif self.cnn_type == 'recognition':\n",
    "                target = target.type(torch.long)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            # _, predicted = torch.max(output, 1)\n",
    "            # total_predictions += labels.size(0)\n",
    "            # correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # average_loss = total_loss / len(validation_data)\n",
    "        # accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        return total_loss / len(validation_data)\n",
    "\n",
    "    def predict(self, test_image):\n",
    "        self.eval()\n",
    "        with torch.inference_mode(mode=True):\n",
    "            test_image = test_image.convert('RGB')\n",
    "            w, h = get_image_size(test_image)\n",
    "            if self.cnn_type == 'detection':\n",
    "                test_image = det_base_transform(test_image)\n",
    "                # test_image = torch.tensor(test_image, dtype=torch.float32)\n",
    "                output = self.forward(test_image.unsqueeze(0))[0]\n",
    "                # print(output)\n",
    "                # output = [output[0], output[1]*w/det_resized[0], output[2]*h/det_resized[1], output[3]*w/det_resized[0], output[4]*h/det_resized[1]]\n",
    "                output = [output[0]*w/det_resized[0], output[1]*h/det_resized[1], output[2]*w/det_resized[0], output[3]*h/det_resized[1]]\n",
    "                # print(output)\n",
    "                return output\n",
    "            elif self.cnn_type == 'recognition':\n",
    "                test_image = rec_val_transform(test_image)\n",
    "                output = self.forward(test_image.unsqueeze(0))\n",
    "                output = np.argmax(output)\n",
    "                if output == 0:\n",
    "                    return -1\n",
    "                return int(output)\n",
    "\n",
    "    def training_loop(self, training_loader, val_loader, epochs, loss_fn, lr):\n",
    "        tr_losses = []\n",
    "        val_losses = []\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        best_weights = self.state_dict().copy()\n",
    "        if self.cnn_type == 'detection':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "        early_stopper = EarlyStopper(patience=3, delta=0.01)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            tr_losses.append(self.fit(training_loader, loss_fn, optimizer))\n",
    "            loss = self.evaluate(val_loader, loss_fn)\n",
    "            val_losses.append(loss)\n",
    "            if self.cnn_type == 'detection':\n",
    "                scheduler.step(loss)\n",
    "\n",
    "            stopper = early_stopper.early_stop(loss)\n",
    "            if stopper == 0:\n",
    "                best_weights = self.state_dict().copy()\n",
    "            elif stopper > early_stopper.patience:\n",
    "                self.load_state_dict(best_weights)\n",
    "                break\n",
    "\n",
    "            if epoch % epoch_step == 0:\n",
    "                print(f'Epoch {epoch} has loss {val_losses[epoch]}')\n",
    "        \n",
    "        return tr_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionData(Dataset):\n",
    "    # the data is in the form [img_name, boundaries]\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.data[idx]\n",
    "        with Image.open(images_path + image_name) as image:\n",
    "            # Apply transformations if specified\n",
    "            image = image.convert('RGB')\n",
    "            w, h = get_image_size(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            # image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "            # label = [label[0], label[1]/w*det_resized[0], label[2]/h*det_resized[1], label[3]/w*det_resized[0], label[4]/h*det_resized[1]]\n",
    "            label = [label[0]/w*det_resized[0], label[1]/h*det_resized[1], label[2]/w*det_resized[0], label[3]/h*det_resized[1]]\n",
    "            label = torch.tensor(label, dtype=torch.float32)\n",
    "            label = torch.flatten(label)\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_train_data, det_val_data, det_test_data = random_split(detection_data, [0.89, 0.1, 0.01], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataset = DetectionData(det_train_data, det_tr_transform)\n",
    "val_dataset = DetectionData(det_val_data, det_base_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_cnn_layers = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    ")\n",
    "detection_fc_layers = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(25088, 32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(32, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = CNN(detection_cnn_layers, detection_fc_layers, 'detection').to(device)\n",
    "print(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection.load_state_dict(torch.load('detection_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in detection.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "det_tr_losses, det_val_losses = detection.training_loop(train_loader, val_loader, epochs1, loss, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_epoch_plot(losses, title='Loss vs training epochs', nbins='auto'):\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=nbins, integer=True))\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    x = np.linspace(1, len(losses)+1, len(losses), dtype=np.uint16)\n",
    "    plt.plot(x, losses)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(det_tr_losses, \"Training loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(det_val_losses, \"Validation loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name, target in det_test_data:\n",
    "    with Image.open(images_path + image_name) as image:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        print(target)\n",
    "        output = detection.predict(image)\n",
    "        pred_fb = patches.Rectangle((output[0], output[1]), output[2]-output[0], output[3]-output[1],\n",
    "                               linewidth=4, edgecolor='red', facecolor='none')\n",
    "        fb = patches.Rectangle((target[0], target[1]), target[2]-target[0], target[3]-target[1],\n",
    "                               linewidth=4, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(pred_fb)\n",
    "        ax.add_patch(fb)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(detection.state_dict(), 'detection_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Detection Model 2 (Viola-Jones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_cv2_model = os.path.dirname(cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "cv2_detector = cv2.CascadeClassifier(path_to_cv2_model)\n",
    "#concatenado de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name, target in det_test_data:\n",
    "    with Image.open(images_path + image_name) as image:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        output = cv2_detector.detectMultiScale(  np.array(image),\n",
    "                                        scaleFactor=1.25,\n",
    "                                        minNeighbors=4,\n",
    "                                        minSize=(60, 60),\n",
    "                                        flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        if len(output) == 1:\n",
    "            output = output[0]\n",
    "        \n",
    "        if len(output) == 4:\n",
    "            pred_fb = patches.Rectangle((output[0], output[1]), output[2]+output[0], output[3]+output[1],\n",
    "                               linewidth=4, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(pred_fb)\n",
    "        fb = patches.Rectangle((target[0], target[1]), target[2]-target[0], target[3]-target[1],\n",
    "                               linewidth=4, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(fb)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recognition model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionData(Dataset):\n",
    "    # the data is in the form [img_name, identity]\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, id, boundary = self.data[idx]\n",
    "        with Image.open(images_path + image_name) as image:\n",
    "            image = image.convert('RGB')\n",
    "            image = image.crop(boundary)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            id = torch.tensor(id, dtype=torch.float32)\n",
    "            return image, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_train_data, rec_val_data, rec_test_data = random_split(recognition_data, [0.89, 0.1, 0.01], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataset2 = RecognitionData(rec_train_data, rec_tr_transform)\n",
    "val_dataset2 = RecognitionData(rec_val_data, rec_val_transform)\n",
    "\n",
    "# sampler = WeightedRandomSampler(class_weights_inv, len(train_dataset2), replacement=True)\n",
    "# train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, sampler=sampler)\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(data, classes):\n",
    "    ids = [entry[1] for entry in data]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(ids, identities)\n",
    "    plt.title('Class counts histogram')\n",
    "    plt.xlabel('Identity')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log', base=2)\n",
    "    plt.show()\n",
    "    counters = np.bincount(ids)\n",
    "    sum = np.sum(counters)\n",
    "    print('Total images:', sum)\n",
    "    return sum / (classes * counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_class_weights(rec_train_data, identities) # Used for balancing classification problem\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "print('Class weights:', class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_cnn_layers = nn.Sequential( # prueba al revés (empezar con numero alto de canales e ir reduciendo hasta 64 64 32 32 16 16)\n",
    "    #64 64 128 128\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2), # prueba de dejarlo en 1 el stride (antes 2)\n",
    "\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "    nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    \n",
    "    nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "    # adaptivemaxpooling prueba\n",
    ")\n",
    "recognition_fc_layers = nn.Sequential(\n",
    "    # nn.Dropout(0.2),# prueba de quitar dropout\n",
    "    nn.Linear(64, 64), # image size que no sea pequeño >12x12\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 81), # 1-80 are ids + (-1) are 81 identities\n",
    "    nn.Softmax(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition = CNN(recognition_cnn_layers, recognition_fc_layers, 'recognition').to(device)\n",
    "summary(recognition, (32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognition.load_state_dict(torch.load('recognition_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in recognition.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = nn.CrossEntropyLoss(class_weights)\n",
    "rec_tr_losses, rec_val_losses = recognition.training_loop(train_loader2, val_loader2, epochs2, loss2, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(rec_tr_losses, \"Training loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(rec_val_losses, \"Validation loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The function that is used to calculate the score in the file CHALL_AGC_FRbasicScript.py'''\n",
    "\n",
    "def CHALL_AGC_ComputeRecognScores(auto_ids, true_ids):\n",
    "    #   Compute face recognition score\n",
    "    #\n",
    "    #   INPUTS\n",
    "    #     - AutomSTR: The results of the automatic face\n",
    "    #     recognition algorithm, stored as an integer\n",
    "    #\n",
    "    #     - AGC_Challenge_STR: The ground truth ids\n",
    "    #\n",
    "    #   OUTPUT\n",
    "    #     - FR_score:     The final recognition score\n",
    "    #\n",
    "    #   --------------------------------------------------------------------\n",
    "    #   AGC Challenge\n",
    "    #   Universitat Pompeu Fabra\n",
    "    #\n",
    "\n",
    "    if len(auto_ids) != len(true_ids):\n",
    "        assert ('Inputs must be of the same len');\n",
    "\n",
    "    f_beta = 1\n",
    "    res_list = list(filter(lambda x: true_ids[x] != -1, range(len(true_ids))))\n",
    "\n",
    "    nTP = len([i for i in res_list if auto_ids[i] == true_ids[i]])\n",
    "\n",
    "    res_list = list(filter(lambda x: auto_ids[x] != -1, range(len(auto_ids))))\n",
    "\n",
    "    nFP = len([i for i in res_list if auto_ids[i] != true_ids[i]])\n",
    "\n",
    "    res_list_auto_ids = list(filter(lambda x: auto_ids[x] == -1, range(len(auto_ids))))\n",
    "    res_list_true_ids = list(filter(lambda x: true_ids[x] != -1, range(len(true_ids))))\n",
    "\n",
    "    nFN = len(set(res_list_auto_ids).intersection(res_list_true_ids))\n",
    "\n",
    "    FR_score = (1 + f_beta ** 2) * nTP / ((1 + f_beta ** 2) * nTP + f_beta ** 2 * nFN + nFP)\n",
    "\n",
    "    return FR_score * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_count = {}\n",
    "expected_count = {}\n",
    "good = 0\n",
    "predictions = []\n",
    "expected = []\n",
    "\n",
    "predictions_count[-1] = 0\n",
    "expected_count[-1] = 0\n",
    "\n",
    "for i in range(1, 81):\n",
    "    predictions_count[i] = 0\n",
    "    expected_count[i] = 0\n",
    "\n",
    "for image_name, id, boundary in rec_train_data:\n",
    "    with Image.open(images_path + image_name) as image:\n",
    "        image = image.convert('RGB')\n",
    "        image = image.crop(boundary)\n",
    "        if id == 0:\n",
    "            target = -1\n",
    "        else:\n",
    "            target = id\n",
    "        output = recognition.predict(image)\n",
    "\n",
    "        predictions_count[output] += 1\n",
    "        expected_count[target] += 1\n",
    "        predictions.append(output)\n",
    "        expected.append(target)\n",
    "\n",
    "        if output == target:\n",
    "            good += 1\n",
    "\n",
    "for key in predictions_count:\n",
    "    print(f'Identity {key}: Predicted {predictions_count[key]} of {expected_count[key]}')\n",
    "\n",
    "print('F1-score:', CHALL_AGC_ComputeRecognScores(predictions, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(recognition.state_dict(), 'recognition_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
