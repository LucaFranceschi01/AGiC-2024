{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection and Recognition CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Luca Franceschi (u199149)\n",
    "\n",
    "In this notebook we will try to construct two CNN models, one for face boundary detection and another for face recognition. The intention for the Coding Challenge 3 is to: with each image first detect if there is a face or not, in case there is, detect the boundaries. Then we can crop the image with the calculated boundaries and pass it to the recognition model to be able to recognize if the face exists in the dataset or is an impostor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io\n",
    "import torch.jit\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from matplotlib import ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms.functional import get_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.001                  # Learning rate\n",
    "epochs = 101                # Total number of epochs\n",
    "batch_size = 32             # The sizes in which the dataset is split to feed the networks\n",
    "det_resized = (224, 224)    # The normalized size of the images in the detection model\n",
    "rec_resized = det_resized   # The normalized size of the images in the recognition model\n",
    "\n",
    "# Other parameters\n",
    "epoch_step = 5              # Frequency in which epoch losses are printed on training\n",
    "images_path = './TRAINING/' # Relative path to the training dataset\n",
    "target_matrix_path = ''     # Relative path to the \"AGC_Challenge3_Training.mat\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print if gpu acceleration is enabled\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the target matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the matrix we can import either the boundaries or the identity. \n",
    "- For the detection network we will import the Image name along with the boundaries. In case there is a face (or more) the first position in the boundaries will be the amount of faces in that image. In case there is no faces, that parameter will be 0 and the boundaries will be a \"padding\" of zeros. The imported data in this case will look like: ```[name, [#faces, x0, y0, x1, y1]]```, very similar as in previous labs. In case there are more than one face the first one in the matrix will be considered.\n",
    "- For the recognition network we will import the image name along with the id and its boundaries. If the id is -1 it is imported as 0 to fix some out of bounds error. This is corrected in the output of the model and is transparent to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(labels_path, labels_wanted='boundaries'):\n",
    "    mat = scipy.io.loadmat(labels_path)['AGC_Challenge3_TRAINING'][0]\n",
    "    data = []\n",
    "    for entry in mat:\n",
    "        key = entry[1][0]\n",
    "        if (len(entry[2]) > 0):\n",
    "            boundary = np.array(entry[2][0], dtype=np.int32)\n",
    "            if (labels_wanted == 'boundaries'):\n",
    "                data.append([key, boundary])\n",
    "            elif (labels_wanted == 'identity'):\n",
    "                if (entry[0][0][0] == -1):\n",
    "                    data.append([key, 0, boundary])\n",
    "                else:\n",
    "                    data.append([key, entry[0][0][0], boundary])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std extracted from the train_dataset part of AGC_Challenge3_TRAINING\n",
    "# def compute_mean_std(dataset):\n",
    "#     all_pixels = []\n",
    "#     for image_path, _ in dataset:\n",
    "#         with Image.open(images_path + image_path) as image:\n",
    "#             image_array = np.array(image)\n",
    "#             all_pixels.append(image_array)\n",
    "\n",
    "#     all_images = np.stack(all_pixels, axis=0)\n",
    "\n",
    "#     mean = np.mean(all_images, axis=(0, 1, 2)) / 255.0\n",
    "#     std_dev = np.std(all_images, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "#     return mean, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_data = load_data(target_matrix_path + 'AGC_Challenge3_Training.mat')\n",
    "recognition_data = load_data(target_matrix_path + 'AGC_Challenge3_Training.mat', 'identity')\n",
    "\n",
    "data_mean = [0.485, 0.456, 0.406]\n",
    "data_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_tr_transform = transforms.Compose([\n",
    "    # transforms.Resize(300, 300),\n",
    "    transforms.RandomResizedCrop(det_resized),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=train_mean, std=train_std)\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])\n",
    "\n",
    "det_base_transform = transforms.Compose([\n",
    "    transforms.Resize(det_resized),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])\n",
    "\n",
    "rec_tr_transform = transforms.Compose([\n",
    "    transforms.Resize(rec_resized),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=data_mean, std=data_std)\n",
    "])\n",
    "\n",
    "rec_val_transform = rec_tr_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Convolutional Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, cnn_layers, fc_layers, type):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.cnn_layers = cnn_layers\n",
    "        self.fc_layers = fc_layers\n",
    "        self.cnn_type = type\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.cnn_layers(data)\n",
    "        output = self.flatten(output) # before linear layer !!!\n",
    "        output = self.fc_layers(output)\n",
    "        return output\n",
    "\n",
    "    def fit(self, training_data, loss_fn, optimizer: optim.Optimizer):\n",
    "        self.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_data, target in training_data:\n",
    "            batch_data = batch_data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self.forward(batch_data)\n",
    "            \n",
    "            if self.cnn_type == 'detection':\n",
    "                pass\n",
    "            elif self.cnn_type == 'recognition':\n",
    "                target = target.type(torch.long)\n",
    "                \n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(training_data)\n",
    "\n",
    "    def evaluate(self, validation_data, loss_fn):\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_data, target in validation_data:\n",
    "            batch_data = batch_data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = self.forward(batch_data)\n",
    "\n",
    "            if self.cnn_type == 'detection':\n",
    "                pass\n",
    "            elif self.cnn_type == 'recognition':\n",
    "                target = target.type(torch.long)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            # _, predicted = torch.max(output, 1)\n",
    "            # total_predictions += labels.size(0)\n",
    "            # correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # average_loss = total_loss / len(validation_data)\n",
    "        # accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        return total_loss / len(validation_data)\n",
    "\n",
    "    def predict(self, test_image):\n",
    "        self.eval()\n",
    "        with torch.inference_mode(mode=True):\n",
    "            test_image = test_image.convert('RGB')\n",
    "            w, h = get_image_size(test_image)\n",
    "            if self.cnn_type == 'detection':\n",
    "                test_image = det_base_transform(test_image)\n",
    "                # test_image = torch.tensor(test_image, dtype=torch.float32)\n",
    "                output = self.forward(test_image.unsqueeze(0))[0]\n",
    "                # print(output)\n",
    "                # output = [output[0], output[1]*w/det_resized[0], output[2]*h/det_resized[1], output[3]*w/det_resized[0], output[4]*h/det_resized[1]]\n",
    "                output = [output[0]*w/det_resized[0], output[1]*h/det_resized[1], output[2]*w/det_resized[0], output[3]*h/det_resized[1]]\n",
    "                # print(output)\n",
    "                return output\n",
    "            elif self.cnn_type == 'recognition':\n",
    "                test_image = rec_val_transform(test_image)\n",
    "                output = self.forward(test_image.unsqueeze(0))\n",
    "                output = np.argmax(output)\n",
    "                if output == 0:\n",
    "                    return -1\n",
    "                return int(output)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Detection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionData(Dataset):\n",
    "    # the data is in the form [img_name, boundaries]\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.data[idx]\n",
    "        with Image.open(images_path + image_name) as image:\n",
    "            # Apply transformations if specified\n",
    "            image = image.convert('RGB')\n",
    "            w, h = get_image_size(image)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            # image = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "            # label = [label[0], label[1]/w*det_resized[0], label[2]/h*det_resized[1], label[3]/w*det_resized[0], label[4]/h*det_resized[1]]\n",
    "            label = [label[0]/w*det_resized[0], label[1]/h*det_resized[1], label[2]/w*det_resized[0], label[3]/h*det_resized[1]]\n",
    "            label = torch.tensor(label, dtype=torch.float32)\n",
    "            label = torch.flatten(label)\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms son copiados, revisar\n",
    "# calcular valores de mean y std del dataset para cada channel\n",
    "# o las de imagenet\n",
    "# flipping, cambios de colores, grayscale\n",
    "# contraste de saturacion \n",
    "# color spaces\n",
    "################\n",
    "\n",
    "det_train_data, det_val_data, det_test_data = random_split(detection_data, [0.89, 0.1, 0.01], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_mean, train_std = compute_mean_std(det_train_data)\n",
    "\n",
    "train_dataset = DetectionData(det_train_data, det_tr_transform)\n",
    "val_dataset = DetectionData(det_val_data, det_base_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_cnn_layers = nn.Sequential(\n",
    "    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    ")\n",
    "detection_fc_layers = nn.Sequential(\n",
    "    nn.Linear(25088, 16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(16, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = CNN(detection_cnn_layers, detection_fc_layers, 'detection').to(device)\n",
    "print(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in detection.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop as train and then test\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(detection.parameters(), lr=lr)\n",
    "\n",
    "det_tr_losses = []\n",
    "det_val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    det_tr_losses.append(detection.fit(train_loader, loss, optimizer))\n",
    "    det_val_losses.append(detection.evaluate(val_loader, loss))\n",
    "\n",
    "    if epoch % epoch_step == 0:\n",
    "        print(f'Epoch {epoch} has loss {det_val_losses[epoch]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_epoch_plot(losses, title='Loss vs training epochs', nbins='auto'):\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=nbins, integer=True))\n",
    "    ax.spines['top'].set_color('white') \n",
    "    ax.spines['right'].set_color('white')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    x = np.linspace(1, len(losses)+1, len(losses), dtype=np.uint16)\n",
    "    plt.plot(x, losses)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(det_tr_losses, \"Training loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(det_val_losses, \"Validation loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_name, target in det_test_data:\n",
    "    with Image.open(images_path + image_name) as image:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        print(target)\n",
    "        output = detection.predict(image)\n",
    "        pred_fb = patches.Rectangle((output[0], output[1]), output[2]-output[0], output[3]-output[1], \n",
    "                               linewidth=4, edgecolor='red', facecolor='none')\n",
    "        fb = patches.Rectangle((target[0], target[1]), target[2]-target[0], target[3]-target[1], \n",
    "                               linewidth=4, edgecolor='green', facecolor='none')\n",
    "        ax.add_patch(pred_fb)\n",
    "        ax.add_patch(fb)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection.eval()\n",
    "detection_scripted = torch.jit.script(detection)\n",
    "torch.jit.save(detection_scripted, 'detection_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recognition model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionData(Dataset):\n",
    "    # the data is in the form [img_name, identity]\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, id, boundary = self.data[idx]\n",
    "        with Image.open(images_path + image_name) as image:\n",
    "            image = image.convert('RGB')\n",
    "            image = image.crop(boundary)\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            id = torch.tensor(id, dtype=torch.float32)\n",
    "            return image, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_train_data, rec_val_data, rec_test_data = random_split(recognition_data, [0.89, 0.1, 0.01], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dataset2 = RecognitionData(rec_train_data, rec_tr_transform)\n",
    "val_dataset2 = RecognitionData(rec_val_data, rec_val_transform)\n",
    "\n",
    "train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
    "val_loader2 = DataLoader(val_dataset2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_cnn_layers = nn.Sequential(\n",
    "    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    ")\n",
    "recognition_fc_layers = nn.Sequential(\n",
    "    nn.Linear(12544, 50),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(50, 81), # 1-80 are ids + (-1) are 81 identities\n",
    "    nn.Softmax(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition = CNN(recognition_cnn_layers, recognition_fc_layers, 'recognition').to(device)\n",
    "print(recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(p.numel() for p in recognition.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# va a sacar un vector y hacer argmax\n",
    "# cross entropy con softmax + adam\n",
    "\n",
    "loss2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adam(recognition.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_tr_losses = []\n",
    "rec_val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    rec_tr_losses.append(recognition.fit(train_loader2, loss2, optimizer2))\n",
    "    rec_val_losses.append(recognition.evaluate(val_loader2, loss2))\n",
    "\n",
    "    if epoch % epoch_step == 0:\n",
    "        print(f'Epoch {epoch} has loss {rec_val_losses[epoch]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(rec_tr_losses, \"Training loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_plot(rec_val_losses, \"Validation loss vs epoch count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = 0\n",
    "bad = 0\n",
    "for image_name, id, boundary in rec_test_data:\n",
    "    with Image.open(images_path + image_name) as image:\n",
    "        image = image.convert('RGB')\n",
    "        image = image.crop(boundary)\n",
    "        if id == 0:\n",
    "            target = -1\n",
    "        else:\n",
    "            target = id\n",
    "        output = recognition.predict(image)\n",
    "\n",
    "        if output == target:\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "print(good, bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition.eval()\n",
    "recognition_scripted = torch.jit.script(recognition)\n",
    "recognition_scripted.save('recognition_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
